# Local Knowledge Base Manager (RAG)

A lightweight, command‚Äëline tool for building a **retrieval‚Äëaugmented generation** (RAG) knowledge base on your local machine.  
It uses:

* **LM Studio** ‚Äì a fast local LLM server that exposes an OpenAI‚Äëcompatible API.
* **ChromaDB** ‚Äì a vector store with HNSW indexing, ideal for similarity search.

> The script works entirely offline; no cloud calls are required.

---

## üì¶ Prerequisites

| Component | Minimum version |
|-----------|-----------------|
| Python     | 3.8+            |
| LM Studio  | ‚â•‚ÄØ0.1 (or any local server that implements the OpenAI API) |
| ChromaDB   | `chromadb==latest` |

### Install dependencies

```bash
# Optional: create a virtual environment first
python -m venv .venv
source .venv/bin/activate      # Windows: .\.venv\Scripts\activate

pip install -r requirements.txt
```

**requirements.txt**

```text
chromadb>=0.4
openai>=1.3
pypdf>=3.17
python-docx>=1.1
tqdm>=4.66
colorama>=0.4
```

---

## ‚öôÔ∏è Configuration

The script reads a few environment variables.  
If you leave them unset, the defaults in `save_to_chroma.py` will be used.

| Variable | Purpose | Default |
|----------|---------|---------|
| `CHROMA_DB_PATH` | Path where ChromaDB stores its data | `G:\AIModels\chroma_db\chroma_db` |
| `DEFAULT_DOCS_DIR` | Folder that is used when the user presses *Enter* on the file‚Äëpath prompt | `G:\Android\ChromaDbDocuments` |
| `LM_STUDIO_URL` | Base URL of the local LM Studio server | `http://localhost:1234/v1` |
| `LM_STUDIO_API_KEY` | API key accepted by LM Studio (usually a dummy value) | `lm-studio` |
| `CHROMA_COLLECTION` | Name of the Chroma collection to use | `my_knowledge_base` |

Example:

```bash
export CHROMA_DB_PATH="/home/user/chroma_db"
export DEFAULT_DOCS_DIR="/home/user/docs"
export LM_STUDIO_URL="http://localhost:1234/v1"
export LM_STUDIO_API_KEY="lm-studio"
```

> **Tip:** If you use Windows PowerShell, prepend `$env:` instead of `export`.

---

## üöÄ Running the script

```bash
python save_to_chroma.py
```

The console shows a menu:

```
=== –ú–µ–Ω–µ–¥–∂–µ—Ä –õ–æ–∫–∞–ª—å–Ω–æ–π –ë–∞–∑—ã –ó–Ω–∞–Ω–∏–π (RAG) ===
1. –ò–Ω–¥–µ–∫—Å–∏—Ä–æ–≤–∞—Ç—å –ø–∞–ø–∫—É (–î–æ–±–∞–≤–∏—Ç—å —Ñ–∞–π–ª—ã)
2. –£–¥–∞–ª–∏—Ç—å —Ç–µ–∫—É—â—É—é –∫–æ–ª–ª–µ–∫—Ü–∏—é (–û—á–∏—Å—Ç–∏—Ç—å –±–∞–∑—É)
3. –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –±–∞–∑–µ
4. –í—ã—Ö–æ–¥
–í—ã–±–µ—Ä–∏—Ç–µ –¥–µ–π—Å—Ç–≤–∏–µ (1-4):
```

### 1Ô∏è‚É£ Index a folder

*Enter the path to a directory containing `.txt`, `.md`, `.pdf` or `.docx` files.*  
If you just press **Enter**, `DEFAULT_DOCS_DIR` is used.

The script:

1. Scans all files in the folder.
2. Reads each file, handling the four supported formats.
3. Splits the text into chunks (`CHUNK_SIZE=1000`, `CHUNK_OVERLAP=200`).
4. For every chunk:
   * Generates an embedding via LM Studio (`text‚Äëembedding-nomic-embed-text-v1.5@q8_0` by default).
   * Stores it in Chroma along with the chunk text and metadata (`source`, `position`).

> **Note:** The chunk ID is a MD5 hash of `<filename><offset>` ‚Äì guarantees uniqueness.

### 2Ô∏è‚É£ Delete current collection

Wipes all documents from the active Chroma collection and recreates an empty one.

### 3Ô∏è‚É£ Database info

Shows:

* Path to the database.
* Collection name.
* Total number of vectors.
* A sample of the first five metadata entries (useful for debugging).

### 4Ô∏è‚É£ Exit

Closes the program.

---

## üìö Behind‚Äëthe‚Äëscenes

| Component | What it does |
|-----------|--------------|
| **LM Studio** | Provides a local OpenAI‚Äëcompatible endpoint (`/v1/embeddings`). The script uses `openai.OpenAI(...)` to call it. |
| **ChromaDB** | Stores vectors, documents and metadata in an HNSW index. All operations are performed through the Python client. |
| **Chunking** | Splits long text into overlapping chunks so that retrieval is fine‚Äëgrained. |
| **Vector generation** | `text-embedding-nomic-embed-text-v1.5@q8_0` (or any model you loaded in LM Studio) ‚Äì returns a 768‚Äëdimensional float32 vector. |

> If LM Studio reports *‚ÄúNo models loaded‚Äù*, load one via the UI or CLI:
> ```bash
> lms load text-embedding-nomic-embed-text-v1.5@q8_0
> ```

---

## üîß Extending / Customising

### Changing the chunk size

Edit `CHUNK_SIZE` and `CHUNK_OVERLAP` near the top of `save_to_chroma.py`.

```python
# Chunking (default: 1000 chars, 200 overlap)
CHUNK_SIZE = 1000
CHUNK_OVERLAP = 200
```

### Using a different embedding model

Set `embedding_model` when creating `VectorDBManager`:

```python
db = VectorDBManager(CHROMA_PATH,
                     collection_name="my_knowledge_base",
                     embedding_model="openai/embedding-ada-002")
```

> Ensure the chosen model is loaded in LM Studio.

### Programmatic usage

You can import the classes and use them directly from another script:

```python
from save_to_chroma import VectorDBManager, FileProcessor

db = VectorDBManager(Path("/path/to/chroma_db"), "my_collection",
                     embedding_model="text-embedding-nomic-embed-text-v1.5@q8_0")

processor = FileProcessor()
text = processor.read_file("some.pdf")
chunks = processor.chunk_text(text, "some.pdf")

db.add_documents(chunks)
```

---

## üìä Querying the collection

While the interactive menu doesn‚Äôt expose a query UI, you can retrieve data in code:

```python
# 1Ô∏è‚É£ By ID
res = db.collection.get(ids=["654c800a7508ec118a5ee9eb66f4a608"])
print(res["documents"][0])

# 2Ô∏è‚É£ By text similarity
query_res = db.collection.query(
    query_texts=["–ò–≥–æ—Ä—å"],
    n_results=3
)
print(query_res["documents"])

# 3Ô∏è‚É£ By metadata filter
res = db.collection.get(where={"source": "about_me.md"}, limit=10)
print(res["metadatas"])
```

---

## üõ†Ô∏è Troubleshooting

| Symptom | Likely cause | Fix |
|---------|--------------|-----|
| `No models loaded` error | LM Studio has no embedding model loaded | Load one via UI or CLI (`lms load ‚Ä¶`) |
| Embedding generation hangs / slow | Model is large or GPU memory limited | Switch to a lighter model, or run on CPU |
| Empty collection after indexing | File path wrong / file unsupported | Verify the folder and file types; check console logs |
| Duplicate IDs / collisions | Same file processed twice | Ensure unique filenames or change chunk ID logic |

---

## üéâ What you get

* A **self‚Äëcontained knowledge base** that can be queried locally.
* Fast similarity search thanks to Chroma‚Äôs HNSW index.
* Zero external dependencies (no cloud, no API keys).
* Ready‚Äëto‚Äëuse example for building a RAG pipeline:
   * Generate embeddings ‚Üí store in Chroma ‚Üí query by text ‚Üí feed results into an LLM.

Happy indexing! üöÄ